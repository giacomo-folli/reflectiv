# Application configuration
# The base URL where your application is accessible
# For Docker: Use http://localhost:3000 when running locally
# For production: Use your actual domain (https://yourdomain.com)
PUBLIC_BASE_URL=http://localhost:3000

# Database configuration
# For local development: ./data
# For Docker: /app/data (this is managed by the Docker volume)
# DATA_DIR=./data

# Node environment
# development, test, or production
NODE_ENV=development

# AI Service Configuration
# Specifies which AI service provider to use. Options: "openai", "gemini", "claude", "local"
# Default is "openai" if not specified or for backward compatibility with old OPENAI_API_KEY setup.
AI_SERVICE_PROVIDER="openai"
# AI_SERVICE_PROVIDER="gemini"
# AI_SERVICE_PROVIDER="claude"
# AI_SERVICE_PROVIDER="local"

# API Keys for AI Services
# Provide the API key for the selected AI_SERVICE_PROVIDER.
# Get your OpenAI API key from https://platform.openai.com/api-keys
OPENAI_API_KEY="your_openai_api_key_here"
# Get your Gemini API key from Google AI Studio
GEMINI_API_KEY="your_gemini_api_key_here"
# Get your Claude API key from Anthropic
CLAUDE_API_KEY="your_claude_api_key_here"

# Optional: Base URIs for AI Services (if you need to override the defaults, e.g., for proxies)
# OPENAI_BASE_URI="your_openai_base_uri_if_not_default"
# GEMINI_BASE_URI="your_gemini_base_uri_if_not_default"
# CLAUDE_BASE_URI="your_claude_base_uri_if_not_default"

# --- Local LLM Configuration ---
# Required if AI_SERVICE_PROVIDER is "local" or "localllm"
# Set this to the base URI of your local LLM API endpoint.
# The LocalLlmService sends a POST request to this URI with JSON: {"prompt": "your prompt", "stream": false}
# It expects a JSON response like {"response": "llm answer"} or an OpenAI-compatible format 
# e.g., {"choices":[{"message":{"content":"..."}}]}.

# Example for Ollama direct generation API (ensure the model is served):
LOCAL_LLM_BASE_URI="http://localhost:11434/api/generate"
# Example for an OpenAI-compatible local server (like LM Studio, LocalAI, or Ollama's OpenAI endpoint):
# LOCAL_LLM_BASE_URI="http://localhost:1234/v1/chat/completions" # Adjust port and path as needed

# Docker-specific environment variables
# These are set in docker-compose.yml but can be overridden in a .env file
# PORT=3000                # The port the application listens on
# HOST=0.0.0.0             # Bind to all interfaces for Docker networking